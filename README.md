# Event-Driven Transactional Sales Data ETL Pipeline on Azure

## ğŸ“– Overview

This project implements an **event-driven, end-to-end ETL pipeline for transactional sales data** using **Azure Data Factory (ADF)** and **Azure Blob Storage**.

Whenever a new transactional sales CSV file is uploaded to the source storage container, a **Storage Event Trigger** automatically activates the pipeline. The pipeline dynamically identifies and processes incoming files, applies transformations using **ADF Mapping Data Flows**, and delivers **analytics-ready sales datasets** to the destination storage layer.

The project simulates a real-world data engineering use case involving **credit card transaction data (Visa, Mastercard, Amex)**, including filtering, enrichment, and aggregation of sales metrics.

---

## ğŸ—ï¸ Architecture Overview

**High-Level Flow:**

1. CSV file uploaded to Source Blob Storage  
2. Storage Event Trigger activates the pipeline  
3. Metadata-driven file validation and selection  
4. Data transformation using Mapping Data Flows  
5. Curated data written to Destination Blob Storage  

ğŸ“· *Architecture / Flow Diagram*  
![Architecture Diagram](images/architecture.png)

---

## âš™ï¸ Technologies Used

- Azure Data Factory  
- Azure Blob Storage  
- Storage Event Triggers  
- ADF Mapping Data Flows  
- CSV-based Transactional Sales Data  

---

## ğŸ”„ Pipeline Workflow

### 1ï¸âƒ£ Event-Based Trigger

The pipeline is automatically triggered whenever a new CSV file is added to the source Blob Storage container, enabling fully automated ingestion without manual intervention.

ğŸ“· *Event Trigger Configuration*  
![Event Trigger](images/event-trigger.png)

---

### 2ï¸âƒ£ Metadata-Driven File Processing

- **Get Metadata** retrieves file details dynamically  
- **ForEach** iterates over incoming files  
- **If Condition** validates and filters eligible CSV files  

ğŸ“· *Get Metadata, ForEach, If Condition Activities*  
![Metadata Processing](images/metadata-foreach.png)

---

### 3ï¸âƒ£ Pipeline Orchestration

- Modular pipeline design using **Execute Pipeline** activities  
- Enables clean separation of ingestion, transformation, and loading logic  

ğŸ“· *Pipeline Orchestration*  
![Execute Pipeline](images/execute-pipeline.png)

---

### 4ï¸âƒ£ Data Transformation (ADF Mapping Data Flow)

The Mapping Data Flow performs the following transformations on transactional sales data:

- Column renaming and schema standardization  
- Filtering invalid or incomplete records  
- Conditional splits based on card type (Visa, Mastercard, Amex)  
- Derived columns for enriched metrics  
- Aggregations to compute sales summaries  

ğŸ“· *Mapping Data Flow Design*  
![Mapping Data Flow](images/mapping-data-flow.png)

---

### 5ï¸âƒ£ Data Movement Across Storage Layers

- Raw transactional data ingested from **Source Container**  
- Intermediate outputs stored in **Reporting Container**  
- Final curated datasets written to **Destination Container**  

ğŸ“· *Source Blob Container*  
![Source Container](images/source-container.png)

ğŸ“· *Destination Blob Container*  
![Destination Container](images/destination-container.png)

---

### 6ï¸âƒ£ Monitoring & Validation

- Pipeline executions monitored using **ADF Monitoring**  
- Debug runs used to validate data flow and logic  
- Ensures reliable and repeatable pipeline execution  

ğŸ“· *ADF Monitoring Dashboard*  
![Pipeline Monitoring](images/pipeline-monitoring.png)

---

## âœ… Key Features

- Event-driven pipeline triggered by file arrival  
- Metadata-driven dynamic file handling  
- Modular and reusable pipeline design  
- Automated end-to-end ETL workflow  
- Analytics-ready transactional sales datasets  
